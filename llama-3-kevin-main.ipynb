{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b29775-f03a-4d6c-9f3a-9715cbb5f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c0b989-5787-4d60-bb88-6f493009fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# Load model on GPU\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,  # or torch.float16 for older GPUs\n",
    "    device_map=\"cuda:0\"  # Automatically uses GPU\n",
    ")\n",
    "model.config.output_attentions = True\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "config = AutoConfig.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a098bb07-ee88-4b24-861e-1acba03b2740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LAYER 0 FORWARD:\n",
      "============================================================\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
      "        **kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
      "        residual = hidden_states\n",
      "\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "\n",
      "        # Self Attention\n",
      "        hidden_states, self_attn_weights = self.self_attn(\n",
      "            hidden_states=hidden_states,\n",
      "            attention_mask=attention_mask,\n",
      "            position_ids=position_ids,\n",
      "            past_key_value=past_key_value,\n",
      "            output_attentions=output_attentions,\n",
      "            use_cache=use_cache,\n",
      "            cache_position=cache_position,\n",
      "            position_embeddings=position_embeddings,\n",
      "            **kwargs,\n",
      "        )\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        # Fully Connected\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "\n",
      "        outputs = (hidden_states,)\n",
      "        if output_attentions:\n",
      "            outputs += (self_attn_weights,)\n",
      "\n",
      "        return outputs\n",
      "\n",
      "\n",
      "============================================================\n",
      "ATTENTION FORWARD:\n",
      "============================================================\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
      "        attention_mask: Optional[torch.Tensor],\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        input_shape = hidden_states.shape[:-1]\n",
      "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
      "\n",
      "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "\n",
      "        cos, sin = position_embeddings\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
      "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
      "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "\n",
      "        attention_interface: Callable = eager_attention_forward\n",
      "        if self.config._attn_implementation != \"eager\":\n",
      "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
      "                logger.warning_once(\n",
      "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
      "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
      "                )\n",
      "            else:\n",
      "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
      "\n",
      "        attn_output, attn_weights = attention_interface(\n",
      "            self,\n",
      "            query_states,\n",
      "            key_states,\n",
      "            value_states,\n",
      "            attention_mask,\n",
      "            dropout=0.0 if not self.training else self.attention_dropout,\n",
      "            scaling=self.scaling,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "        return attn_output, attn_weights\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROTARY EMBEDDING:\n",
      "============================================================\n",
      "class LlamaRotaryEmbedding(nn.Module):\n",
      "    def __init__(self, config: LlamaConfig, device=None):\n",
      "        super().__init__()\n",
      "        # BC: \"rope_type\" was originally \"type\"\n",
      "        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n",
      "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
      "        else:\n",
      "            self.rope_type = \"default\"\n",
      "        self.max_seq_len_cached = config.max_position_embeddings\n",
      "        self.original_max_seq_len = config.max_position_embeddings\n",
      "\n",
      "        self.config = config\n",
      "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
      "\n",
      "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
      "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
      "        self.original_inv_freq = self.inv_freq\n",
      "\n",
      "    def _dynamic_frequency_update(self, position_ids, device):\n",
      "        \"\"\"\n",
      "        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n",
      "        1 - growing beyond the cached sequence length (allow scaling)\n",
      "        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n",
      "        \"\"\"\n",
      "        seq_len = torch.max(position_ids) + 1\n",
      "        if seq_len > self.max_seq_len_cached:  # growth\n",
      "            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n",
      "            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n",
      "            self.max_seq_len_cached = seq_len\n",
      "\n",
      "        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n",
      "            # This .to() is needed if the model has been moved to a device after being initialized (because\n",
      "            # the buffer is automatically moved, but not the original copy)\n",
      "            self.original_inv_freq = self.original_inv_freq.to(device)\n",
      "            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n",
      "            self.max_seq_len_cached = self.original_max_seq_len\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def forward(self, x, position_ids):\n",
      "        if \"dynamic\" in self.rope_type:\n",
      "            self._dynamic_frequency_update(position_ids, device=x.device)\n",
      "\n",
      "        # Core RoPE block\n",
      "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
      "        position_ids_expanded = position_ids[:, None, :].float()\n",
      "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
      "        device_type = x.device.type\n",
      "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
      "        with torch.autocast(device_type=device_type, enabled=False):\n",
      "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
      "            emb = torch.cat((freqs, freqs), dim=-1)\n",
      "            cos = emb.cos()\n",
      "            sin = emb.sin()\n",
      "\n",
      "        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
      "        cos = cos * self.attention_scaling\n",
      "        sin = sin * self.attention_scaling\n",
      "\n",
      "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "MODEL.MODEL FORWARD:\n",
      "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: torch.LongTensor = None,\n",
      "        attention_mask: Optional[torch.Tensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Cache] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
      "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
      "\n",
      "        if self.gradient_checkpointing and self.training and use_cache:\n",
      "            logger.warning_once(\n",
      "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
      "            )\n",
      "            use_cache = False\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "\n",
      "        if use_cache and past_key_values is None:\n",
      "            past_key_values = DynamicCache()\n",
      "\n",
      "        if cache_position is None:\n",
      "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
      "            cache_position = torch.arange(\n",
      "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
      "            )\n",
      "\n",
      "        if position_ids is None:\n",
      "            position_ids = cache_position.unsqueeze(0)\n",
      "\n",
      "        causal_mask = self._update_causal_mask(\n",
      "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
      "        )\n",
      "\n",
      "        hidden_states = inputs_embeds\n",
      "\n",
      "        # create position embeddings to be shared across the decoder layers\n",
      "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
      "\n",
      "        # decoder layers\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        all_self_attns = () if output_attentions else None\n",
      "\n",
      "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states += (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "                layer_outputs = self._gradient_checkpointing_func(\n",
      "                    decoder_layer.__call__,\n",
      "                    hidden_states,\n",
      "                    causal_mask,\n",
      "                    position_ids,\n",
      "                    past_key_values,\n",
      "                    output_attentions,\n",
      "                    use_cache,\n",
      "                    cache_position,\n",
      "                    position_embeddings,\n",
      "                )\n",
      "            else:\n",
      "                layer_outputs = decoder_layer(\n",
      "                    hidden_states,\n",
      "                    attention_mask=causal_mask,\n",
      "                    position_ids=position_ids,\n",
      "                    past_key_value=past_key_values,\n",
      "                    output_attentions=output_attentions,\n",
      "                    use_cache=use_cache,\n",
      "                    cache_position=cache_position,\n",
      "                    position_embeddings=position_embeddings,\n",
      "                    **flash_attn_kwargs,\n",
      "                )\n",
      "\n",
      "            hidden_states = layer_outputs[0]\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attns += (layer_outputs[1],)\n",
      "\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "\n",
      "        # add hidden states from the last decoder layer\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states += (hidden_states,)\n",
      "\n",
      "        output = BaseModelOutputWithPast(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=past_key_values if use_cache else None,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attns,\n",
      "        )\n",
      "        return output if return_dict else output.to_tuple()\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "MODEL.MODEL embedding:\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.embedding(\n",
      "            input,\n",
      "            self.weight,\n",
      "            self.padding_idx,\n",
      "            self.max_norm,\n",
      "            self.norm_type,\n",
      "            self.scale_grad_by_freq,\n",
      "            self.sparse,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import inspect\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "layer0 = model.model.layers[0]\n",
    "\n",
    "# View Layer 0 forward method\n",
    "print(\"=\"*60)\n",
    "print(\"LAYER 0 FORWARD:\")\n",
    "print(\"=\"*60)\n",
    "print(inspect.getsource(layer0.forward))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ATTENTION FORWARD:\")\n",
    "print(\"=\"*60)\n",
    "print(inspect.getsource(layer0.self_attn.forward))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ROTARY EMBEDDING:\")\n",
    "print(\"=\"*60)\n",
    "print(inspect.getsource(model.model.rotary_emb.__class__))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nMODEL.MODEL FORWARD:\")\n",
    "print(inspect.getsource(model.model.forward))\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nMODEL.MODEL embedding:\")\n",
    "print(inspect.getsource(model.model.embed_tokens.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871f0025-a07e-4dbc-8046-bd290e3f4bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1e34ed-e6dd-4c73-aa18-8821836137ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREFILL PHASE\n",
      "============================================================\n",
      "Input text: 'The future of artificial intelligence is'\n",
      "Input shape: torch.Size([1, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 7, 64])\n",
      "Predicted next token: ' here'\n",
      "KV cache created for 16 layers\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PREFILL PHASE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Input text: '{input_text}'\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        use_cache=True,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "past_key_values = outputs.past_key_values\n",
    "print(past_key_values.key_cache[0].shape)\n",
    "next_token_logits = outputs.logits[:, -1, :]\n",
    "next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "print(f\"Predicted next token: '{tokenizer.decode(next_token_id[0])}'\")\n",
    "print(f\"KV cache created for {len(past_key_values)} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c60c5b-ed87-4439-89be-6638b5228747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with hooks to capture ground truth...\n",
      "Position IDs shape: torch.Size([1, 1])\n",
      "Position IDs:\n",
      "tensor([[7]], device='cuda:0')\n",
      "Output cos shape: torch.Size([1, 1, 64])\n",
      "Output sin shape: torch.Size([1, 1, 64])\n",
      "PRE HOOK INPUTS:  ()\n"
     ]
    }
   ],
   "source": [
    "ground_truth = {}\n",
    "\n",
    "def attention_pre_hook(module, input):\n",
    "    \"\"\"\n",
    "    Captures inputs to attention forward BEFORE computation\n",
    "    input is a tuple of all arguments to forward()\n",
    "    \"\"\"\n",
    "    # input[0] = hidden_states\n",
    "    # input[1] = position_embeddings (tuple of cos, sin)\n",
    "    # input[2] = attention_mask\n",
    "    # input[3] = past_key_value\n",
    "    # input[4] = cache_position\n",
    "    # input[5+] = kwargs\n",
    "    \n",
    "    if len(input) > 0:\n",
    "        ground_truth['attn_hidden_states'] = input[0].detach().clone()\n",
    "    if len(input) > 1:\n",
    "        ground_truth['attn_position_embeddings'] = input[1]  # This is (cos, sin) tuple\n",
    "    if len(input) > 2:\n",
    "        ground_truth['attn_attention_mask'] = input[2].detach().clone() if input[2] is not None else None\n",
    "    if len(input) > 3:\n",
    "        ground_truth['attn_past_key_value'] = input[3]\n",
    "    if len(input) > 4:\n",
    "        ground_truth['attn_cache_position'] = input[4].detach().clone() if input[4] is not None else None\n",
    "    \n",
    "    print(\"PRE HOOK INPUTS: \", input)\n",
    "def rotary_embedding_hook(module, input, output):\n",
    "    if isinstance(output, tuple) and len(output) == 2:\n",
    "        # Rotary embeddings return (cos, sin)\n",
    "        ground_truth['rot_emb_cos'] = output[0].detach().clone()\n",
    "        ground_truth['rot_emb_sin'] = output[1].detach().clone()\n",
    "    else:\n",
    "        ground_truth['rot_emb_out'] = output.detach().clone()\n",
    "\n",
    "\"\"\"\n",
    "def make_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            ground_truth[f\"{name}_output\"] = output[0].detach().clone()\n",
    "        else:\n",
    "            ground_truth[f\"{name}_output\"] = output.detach().clone()\n",
    "        if isinstance(input, tuple):\n",
    "            ground_truth[f\"{name}_input\"] = input[0].detach().clone()\n",
    "        else:\n",
    "            ground_truth[f\"{name}_input\"] = input.detach().clone()\n",
    "    return hook\n",
    "    \"\"\"\n",
    "def make_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        # Handle output - can be tuple or tensor\n",
    "        if isinstance(output, tuple):\n",
    "            ground_truth[f\"{name}_output\"] = output[0].detach().clone()\n",
    "            ground_truth[f\"{name}_output2\"] = output[1].detach().clone()\n",
    "\n",
    "        else:\n",
    "            ground_truth[f\"{name}_output\"] = output.detach().clone()\n",
    "        \n",
    "        # Handle input - ALWAYS a tuple in forward hooks\n",
    "        # input[0] is the first argument (usually the tensor)\n",
    "        if isinstance(input, tuple) and len(input) > 0:\n",
    "            if isinstance(input[0], torch.Tensor):\n",
    "                ground_truth[f\"{name}_input\"] = input[0].detach().clone()\n",
    "            else:\n",
    "                # If it's not a tensor, skip it\n",
    "                pass\n",
    "        else: \n",
    "            ground_truth[f\"{name}_input\"] = input\n",
    "        \n",
    "    return hook\n",
    "def make_input_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(input, tuple):\n",
    "            ground_truth[name] = input[0].detach().clone()\n",
    "        else:\n",
    "            ground_truth[name] = input.detach().clone()\n",
    "    return hook\n",
    "def rope_hook(module, input, output):\n",
    "    x, position_ids = input\n",
    "    ground_truth['rot_emb_position_ids'] = position_ids.detach().clone()\n",
    "    ground_truth['rot_emb_x'] = x.detach().clone()\n",
    "\n",
    "    print(f\"Position IDs shape: {position_ids.shape}\")\n",
    "    print(f\"Position IDs:\\n{position_ids}\")\n",
    "    print(f\"Output cos shape: {output[0].shape}\")\n",
    "    print(f\"Output sin shape: {output[1].shape}\")\n",
    "    return output\n",
    "\n",
    "# Also capture intermediate attention states\n",
    "attention_internals = {}\n",
    "\n",
    "def attention_hook(module, input, output):\n",
    "    # Capture the full output tuple from attention\n",
    "    if isinstance(output, tuple):\n",
    "        attention_internals['attn_output'] = output[0].detach().clone()\n",
    "        if len(output) > 1 and output[1] is not None:\n",
    "            attention_internals['attn_weights'] = output[1].detach().clone() if output[1] is not None else None\n",
    "        if len(output) > 2:\n",
    "            attention_internals['past_kv'] = output[2]\n",
    "    else:\n",
    "        attention_internals['attn_output'] = output.detach().clone()\n",
    "\n",
    "# Register hooks on Layer 0\n",
    "layer_0 = model.model.layers[0]\n",
    "hooks = []\n",
    "\n",
    "# Capture embedding\n",
    "def embedding_hook(module, input, output):\n",
    "    ground_truth['embedding_output'] = output.detach().clone()\n",
    "    ground_truth['embedding_input'] = input.detach().clone()\n",
    "\n",
    "\n",
    "\n",
    "#hooks.append(model.model.embed_tokens.register_forward_hook(embedding_hook))\n",
    "hooks.append(model.model.rotary_emb.register_forward_hook(rotary_embedding_hook))\n",
    "\n",
    "hooks.append(model.model.rotary_emb.register_forward_hook(make_input_hook(\"rot_emb_in\")))\n",
    "\n",
    "hooks.append(layer_0.input_layernorm.register_forward_hook(make_hook('input_layernorm')))\n",
    "hooks.append(layer_0.self_attn.q_proj.register_forward_hook(make_hook('q_proj')))\n",
    "hooks.append(layer_0.self_attn.k_proj.register_forward_hook(make_hook('k_proj')))\n",
    "hooks.append(layer_0.self_attn.v_proj.register_forward_hook(make_hook('v_proj')))\n",
    "hooks.append(layer_0.self_attn.register_forward_hook(make_hook(\"attention\")))\n",
    "hooks.append(layer_0.self_attn.o_proj.register_forward_hook(make_hook('o_proj')))\n",
    "#hooks.append(layer_0.self_attn.config.attn_implementation(make_hook('atten_interface')))\n",
    "#hooks.append(model.model._update_causal_mask.register_forward_hook(make_hook('causal_mask')))\n",
    "hooks.append(layer_0.self_attn.register_forward_pre_hook(attention_pre_hook))\n",
    "\n",
    "\n",
    "hooks.append(layer_0.post_attention_layernorm.register_forward_hook(make_hook('post_attention_layernorm')))\n",
    "hooks.append(layer_0.mlp.gate_proj.register_forward_hook(make_hook('gate_proj')))\n",
    "hooks.append(layer_0.mlp.up_proj.register_forward_hook(make_hook('up_proj')))\n",
    "hooks.append(layer_0.mlp.down_proj.register_forward_hook(make_hook('down_proj')))\n",
    "hooks.append(model.model.rotary_emb.register_forward_hook(rope_hook))\n",
    "\n",
    "# Capture residual connection after attention\n",
    "def layer_output_hook(module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        ground_truth['layer_0_output'] = output[0].detach().clone()\n",
    "    else:\n",
    "        ground_truth['layer_0_output'] = output.detach().clone()\n",
    "\n",
    "hooks.append(layer_0.register_forward_hook(layer_output_hook))\n",
    "\n",
    "# Run model with hooks to get ground truth\n",
    "decode_token_id = next_token_id.unsqueeze(0)\n",
    "print(f\"\\nRunning model with hooks to capture ground truth...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    ground_truth_outputs = model(\n",
    "        input_ids=decode_token_id,\n",
    "        attention_mask=torch.ones_like(decode_token_id),\n",
    "        past_key_values=past_key_values,\n",
    "        use_cache=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "# Remove hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "087045cb-64d0-4d37-83b3-3b57b0fb709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth captured for 27 operations\n",
      "Ground truth keys: ['rot_emb_cos', 'rot_emb_sin', 'rot_emb_in', 'rot_emb_position_ids', 'rot_emb_x', 'input_layernorm_output', 'input_layernorm_input', 'q_proj_output', 'q_proj_input', 'k_proj_output', 'k_proj_input', 'v_proj_output', 'v_proj_input', 'o_proj_output', 'o_proj_input', 'attention_output', 'attention_output2', 'attention_input', 'post_attention_layernorm_output', 'post_attention_layernorm_input', 'gate_proj_output', 'gate_proj_input', 'up_proj_output', 'up_proj_input', 'down_proj_output', 'down_proj_input', 'layer_0_output']\n",
      "Attention internals keys: []\n",
      "torch.Size([1, 1, 64])\n",
      "torch.Size([1, 1, 64])\n",
      "torch.Size([1, 1, 2048])\n",
      "Tokens: [128000, 1618]\n",
      "First token: 1618\n",
      "Embedding matrix shape: torch.Size([128256, 2048])\n",
      "tensor([ 0.0088,  0.0013,  0.0354,  ..., -0.0277, -0.0461,  0.0244],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>)\n",
      "torch.Size([128256, 2048])\n",
      "sdpa\n",
      "True\n",
      "attention outs: (1, 32, 1, 8)\n",
      "position ids: [[7]]\n",
      "rot embedding x input vector shape:  torch.Size([1, 1, 2048])\n",
      "rot embedding x INPUT vector:  [[[ 0.00878906  0.00130463  0.03540039 ... -0.02770996 -0.04614258\n",
      "    0.02441406]]]\n"
     ]
    }
   ],
   "source": [
    "# PRINTING STUFF HOOKED FROM MODEL...\n",
    "\n",
    "print(f\"Ground truth captured for {len(ground_truth)} operations\")\n",
    "print(f\"Ground truth keys: {list(ground_truth.keys())}\")\n",
    "print(f\"Attention internals keys: {list(attention_internals.keys())}\")\n",
    "\n",
    "print(ground_truth['rot_emb_cos'].shape) # *** K.K  we probably want to use this as the rotary embedding weights...\n",
    "print(ground_truth['rot_emb_sin'].shape) # *** K.K  we probably want to use this as the rotary embedding weights...\n",
    "\n",
    "print(ground_truth['rot_emb_in'].shape)\n",
    "sin_golden = ground_truth['rot_emb_sin']\n",
    "cos_golden = ground_truth['rot_emb_cos']\n",
    "\n",
    "text_with_space = \" here\"\n",
    "tokens = tokenizer.encode(text_with_space)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"First token: {tokens[1]}\")\n",
    "\n",
    "# Get embedding matrix\n",
    "embedding_matrix = model.model.embed_tokens.weight\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "\n",
    "print(embedding_matrix[1618])\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "print(layer_0.self_attn.config._attn_implementation)\n",
    "\n",
    "print(model.model.config.output_attentions)\n",
    "\n",
    "print(\"attention outs:\" , ground_truth['attention_output2'].float().cpu().numpy().shape)\n",
    "#print(\"o proj outs:\" , ground_truth['o_proj_output'].float().cpu().numpy().shape)\n",
    "\n",
    "\n",
    "print(\"position ids:\" , ground_truth['rot_emb_position_ids'].cpu().numpy())\n",
    "print(\"rot embedding x input vector shape: \", ground_truth['rot_emb_x'].shape)\n",
    "print(\"rot embedding x INPUT vector: \", ground_truth['rot_emb_x'].float().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b00b91c-18f8-4408-b1fe-4fa3b321a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DECODE PHASE - LAYER 0 DETAILED INSPECTION\n",
      "============================================================\n",
      "\n",
      "Decode token ID: tensor([[1618]], device='cuda:0')\n",
      "Decode token: ' here'\n",
      "\n",
      "============================================================\n",
      "STEP 1: Token Embedding\n",
      "============================================================\n",
      "Embedding output shape: torch.Size([1, 1, 2048])\n",
      "Embedding dtype: torch.bfloat16\n",
      "Embedding sample values: tensor([ 0.0088,  0.0013,  0.0354,  0.0029, -0.0430], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "  ⚠️  No ground truth available for Token Embedding\n",
      "torch.Size([1, 1, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DECODE PHASE - LAYER 0 DETAILED INSPECTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nDecode token ID: {decode_token_id}\")\n",
    "print(f\"Decode token: '{tokenizer.decode(decode_token_id[0])}'\")\n",
    "\n",
    "# Helper function to compare tensors\n",
    "def compare_tensors(manual, ground_truth, name, rtol=1e-3, atol=1e-3\n",
    "                   ):\n",
    "    if ground_truth is None:\n",
    "        print(f\"  ⚠️  No ground truth available for {name}\")\n",
    "        return\n",
    "    \n",
    "    manual = manual.to(ground_truth.dtype)\n",
    "    match = torch.allclose(manual, ground_truth, rtol=rtol, atol=atol)\n",
    "    max_diff = (manual - ground_truth).abs().max().item()\n",
    "    mean_diff = (manual - ground_truth).abs().mean().item()\n",
    "    \n",
    "    if match:\n",
    "        print(f\"  ✓ MATCH: {name}\")\n",
    "    else:\n",
    "        print(f\"  ✗ MISMATCH: {name}\")\n",
    "    print(f\"    Max diff: {max_diff:.6e}, Mean diff: {mean_diff:.6e}\")\n",
    "    print(f\"    Manual shape: {manual.shape}, GT shape: {ground_truth.shape}\")\n",
    "    print(f\"    Manual sample: {manual.flatten()[:3]}\")\n",
    "    print(f\"    GT sample:     {ground_truth.flatten()[:3]}\")\n",
    "\n",
    "# Step 1: Get embedding for the new token\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 1: Token Embedding\")\n",
    "print(f\"{'='*60}\")\n",
    "embed_tokens = model.model.embed_tokens\n",
    "hidden_states = embed_tokens(decode_token_id)\n",
    "print(f\"Embedding output shape: {hidden_states.shape}\")\n",
    "print(f\"Embedding dtype: {hidden_states.dtype}\")\n",
    "print(f\"Embedding sample values: {hidden_states[0, 0, :5]}\")\n",
    "compare_tensors(hidden_states, ground_truth.get('embedding'), \"Token Embedding\")\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4909527d-22a3-4b93-abd9-1fc3b7e188bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: Input Layer Norm\n",
      "============================================================\n",
      "[('weight', Parameter containing:\n",
      "tensor([0.1582, 0.1807, 0.2695,  ..., 0.2217, 0.2109, 0.1523], device='cuda:0',\n",
      "       dtype=torch.bfloat16, requires_grad=True))]\n",
      "tensor([[[0.0005]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<MeanBackward1>) AHPPY\n",
      "  ✗ MISMATCH: lnorm\n",
      "    Max diff: 1.562500e-02, Mean diff: 2.784729e-04\n",
      "    Manual shape: torch.Size([1, 1, 2048]), GT shape: torch.Size([1, 1, 2048])\n",
      "    Manual sample: tensor([0.0603, 0.0103, 0.4121], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "    GT sample:     tensor([0.0603, 0.0103, 0.4141], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Input Layer Norm\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 2: Input Layer Norm\")\n",
    "print(f\"{'='*60}\")\n",
    "input_layernorm = layer_0.input_layernorm\n",
    "print(list(input_layernorm.named_parameters()))\n",
    "normed_hidden = input_layernorm(hidden_states)\n",
    "hidden_states_sq = hidden_states ** 2\n",
    "mean = hidden_states_sq.mean(dim=-1,keepdim=True)\n",
    "print(mean, \"AHPPY\")\n",
    "x_norm = (hidden_states)/(torch.sqrt(mean +1e-05)) * input_layernorm.weight\n",
    "compare_tensors(x_norm, normed_hidden, \"lnorm\")\n",
    "#print(norm.shape)\n",
    "#print(f\"LayerNorm output shape: {normed_hidden.shape}\")\n",
    "#print(f\"LayerNorm output sample: {normed_hidden[0, 0, :5]}\")\n",
    "#compare_tensors(normed_hidden, ground_truth.get('input_layernorm'), \"Input LayerNorm\")\n",
    "\n",
    "#K.K. says this part done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1bff17-26a1-45d9-bb6c-cd28f2d0bda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: Q, K, V Projections\n",
      "============================================================\n",
      "[('q_proj.weight', Parameter containing:\n",
      "tensor([[-0.0183,  0.0071,  0.0219,  ..., -0.0070, -0.0089,  0.0149],\n",
      "        [ 0.0112,  0.0593,  0.0630,  ..., -0.0334, -0.0148,  0.0058],\n",
      "        [ 0.0182,  0.0141,  0.0361,  ..., -0.0432, -0.0388, -0.0233],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0289,  0.0801,  ..., -0.0767, -0.0311, -0.0334],\n",
      "        [ 0.0242, -0.0325,  0.0369,  ..., -0.0123, -0.0269, -0.0151],\n",
      "        [-0.0264, -0.0498, -0.0210,  ...,  0.0601,  0.0130, -0.0007]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)), ('k_proj.weight', Parameter containing:\n",
      "tensor([[ 0.0559,  0.1123,  0.0718,  ..., -0.1045, -0.0332,  0.0104],\n",
      "        [-0.0126,  0.0718,  0.0645,  ..., -0.0304, -0.0206, -0.0496],\n",
      "        [-0.0820, -0.0408, -0.0036,  ...,  0.0036, -0.0237,  0.0013],\n",
      "        ...,\n",
      "        [-0.0012,  0.0042,  0.0047,  ..., -0.0003, -0.0332, -0.0099],\n",
      "        [-0.0261, -0.0064,  0.0422,  ...,  0.0359,  0.0236,  0.0057],\n",
      "        [ 0.0179,  0.0250,  0.0133,  ...,  0.0024,  0.0297,  0.0304]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)), ('v_proj.weight', Parameter containing:\n",
      "tensor([[ 0.0146, -0.0002,  0.0101,  ...,  0.0062, -0.0040, -0.0069],\n",
      "        [ 0.0066, -0.0022, -0.0097,  ...,  0.0006,  0.0055, -0.0052],\n",
      "        [-0.0008,  0.0047, -0.0051,  ...,  0.0019,  0.0030,  0.0053],\n",
      "        ...,\n",
      "        [-0.0066,  0.0022,  0.0003,  ..., -0.0089, -0.0136,  0.0031],\n",
      "        [-0.0082, -0.0045, -0.0020,  ..., -0.0052, -0.0012,  0.0164],\n",
      "        [-0.0166, -0.0045,  0.0009,  ..., -0.0074, -0.0071, -0.0081]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)), ('o_proj.weight', Parameter containing:\n",
      "tensor([[ 3.3760e-04,  6.5002e-03,  1.2634e-02,  ..., -7.7820e-03,\n",
      "         -1.5106e-03,  1.1414e-02],\n",
      "        [ 1.0925e-02,  7.5073e-03, -5.7373e-03,  ..., -6.0272e-04,\n",
      "         -7.7057e-04, -6.8054e-03],\n",
      "        [-4.0283e-03, -4.7363e-02, -1.9165e-02,  ..., -9.8267e-03,\n",
      "         -2.5177e-03, -2.5635e-03],\n",
      "        ...,\n",
      "        [ 6.9580e-03, -1.3550e-02,  2.7618e-03,  ...,  8.5449e-03,\n",
      "          6.1035e-03, -6.5918e-03],\n",
      "        [ 8.3542e-04, -2.3560e-02,  3.2234e-04,  ..., -9.1553e-03,\n",
      "         -7.6904e-03, -5.0049e-03],\n",
      "        [-2.5940e-03,  1.0376e-02, -1.2634e-02,  ..., -8.2016e-05,\n",
      "          1.8005e-03, -2.9945e-04]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       requires_grad=True))] ARCH\n",
      "None\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "[('weight', Parameter containing:\n",
      "tensor([[-0.0183,  0.0071,  0.0219,  ..., -0.0070, -0.0089,  0.0149],\n",
      "        [ 0.0112,  0.0593,  0.0630,  ..., -0.0334, -0.0148,  0.0058],\n",
      "        [ 0.0182,  0.0141,  0.0361,  ..., -0.0432, -0.0388, -0.0233],\n",
      "        ...,\n",
      "        [ 0.0305,  0.0289,  0.0801,  ..., -0.0767, -0.0311, -0.0334],\n",
      "        [ 0.0242, -0.0325,  0.0369,  ..., -0.0123, -0.0269, -0.0151],\n",
      "        [-0.0264, -0.0498, -0.0210,  ...,  0.0601,  0.0130, -0.0007]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True))]\n",
      "torch.Size([2048, 2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([2048, 1, 1])\n",
      "  ✗ MISMATCH: Q Projection\n",
      "    Max diff: 6.250000e-02, Mean diff: 3.753662e-03\n",
      "    Manual shape: torch.Size([1, 1, 2048]), GT shape: torch.Size([2048])\n",
      "    Manual sample: tensor([1.0547, 2.8438, 2.5781], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "    GT sample:     tensor([1.0547, 2.8438, 2.5781], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "  ✗ MISMATCH: K Projection\n",
      "    Max diff: 6.250000e-02, Mean diff: 4.028320e-03\n",
      "    Manual shape: torch.Size([1, 1, 512]), GT shape: torch.Size([512])\n",
      "    Manual sample: tensor([ 4.8438,  2.9219, -1.2734], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "    GT sample:     tensor([ 4.8125,  2.9219, -1.2656], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "  ✗ MISMATCH: V Projection\n",
      "    Max diff: 1.953125e-03, Mean diff: 2.288818e-04\n",
      "    Manual shape: torch.Size([1, 1, 512]), GT shape: torch.Size([512])\n",
      "    Manual sample: tensor([-0.0111,  0.0618, -0.0505], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "    GT sample:     tensor([-0.0111,  0.0615, -0.0505], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319708/4138388599.py:15: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1739474893324/work/aten/src/ATen/native/TensorShape.cpp:3725.)\n",
      "  print(x_norm.T.shape)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Self-Attention - Q, K, V Projections\n",
    "import inspect\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 3: Q, K, V Projections\")\n",
    "print(f\"{'='*60}\")\n",
    "self_attn = layer_0.self_attn\n",
    "print(list(self_attn.named_parameters()), \"ARCH\")\n",
    "\n",
    "print(self_attn.q_proj.bias)\n",
    "print(inspect.getsource(self_attn.q_proj.forward))\n",
    "print(list(self_attn.q_proj.named_parameters()))\n",
    "\n",
    "print(self_attn.q_proj.weight.shape)\n",
    "print(self_attn.k_proj.weight.shape)\n",
    "print(x_norm.T.shape)\n",
    "#KEVIN STYLE\n",
    "q_kevin = x_norm.reshape(2048) @ self_attn.q_proj.weight.T \n",
    "k_kevin = self_attn.k_proj.weight @ x_norm.reshape(2048).T\n",
    "v_kevin = x_norm.reshape(2048) @ self_attn.v_proj.weight.T\n",
    "\n",
    "# Project to Q, K, V\n",
    "q = self_attn.q_proj(normed_hidden)\n",
    "k = self_attn.k_proj(normed_hidden)\n",
    "v = self_attn.v_proj(normed_hidden)\n",
    "compare_tensors(q, q_kevin, \"Q Projection\")\n",
    "compare_tensors(k, k_kevin, \"K Projection\")\n",
    "compare_tensors(v, v_kevin, \"V Projection\")\n",
    "\n",
    "#print(f\"Q shape: {q.shape}\")\n",
    "#print(f\"K shape: {k.shape}\")\n",
    "#print(f\"V shape: {v.shape}\")\n",
    "#compare_tensors(q, ground_truth.get('q_proj'), \"Q Projection\")\n",
    "#compare_tensors(k, ground_truth.get('k_proj'), \"K Projection\")\n",
    "#compare_tensors(v, ground_truth.get('v_proj'), \"V Projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b1e327-3c4b-45b0-8caa-d53971b183a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: Reshape for Multi-Head Attention\n",
      "============================================================\n",
      "Config values:\n",
      "  num_attention_heads: 32\n",
      "  num_key_value_heads: 8\n",
      "  hidden_size: 2048\n",
      "  head_dim: 64\n",
      "\n",
      "Q reshaped: torch.Size([1, 32, 1, 64]) [batch, num_heads, seq_len, head_dim]\n",
      "K reshaped: torch.Size([1, 8, 1, 64]) [batch, num_kv_heads, seq_len, head_dim]\n",
      "V reshaped: torch.Size([1, 8, 1, 64]) [batch, num_kv_heads, seq_len, head_dim]\n",
      "Q reshaped sample: tensor([ 1.0547,  2.8438,  2.5781, -3.6094, -4.9062], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "K reshaped sample: tensor([4.8438], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "V reshaped sample: tensor([-0.0111,  0.0618, -0.0505, -0.0020, -0.0245], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Q kevin reshaped sample: tensor([[  1.0547,   2.8438,   2.5781,  -3.5938,  -4.9062,  -1.0156,  -1.4062,\n",
      "          -0.9688,  -1.7891,  -2.6562,   1.4844,   2.1719,  -2.3594,  -0.9297,\n",
      "          -0.9062,  -2.6406,   1.9453,   4.5938,  -0.0205,  -0.5078, -13.1875,\n",
      "          -6.9062,  -0.1699,   1.7500,   0.0679,  -0.8203,  -2.5625,   1.3047,\n",
      "          -2.4219,  -1.4141,   1.5703,  -3.2031,  -0.0156,  -1.3438,   1.4609,\n",
      "           3.4531,   0.4570,   6.3438,  -2.7500,   3.3281,  -1.7109,  -1.2031,\n",
      "           2.6250,  -1.7734,   1.2266,  -2.3594,   0.5312,   0.8047,  -0.5664,\n",
      "           0.6172,  -0.0258,  -5.7812,  -1.1875,  -1.2812,   2.2969,   0.8086,\n",
      "          -1.2188,   1.0859,   0.1992,  -0.0718,  -0.6953,  -1.2266,   0.7734,\n",
      "           1.1172]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "K kevin reshaped sample: tensor([[ 4.8125],\n",
      "        [ 0.1187],\n",
      "        [ 0.7422],\n",
      "        [-0.8984],\n",
      "        [-0.8711]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "V kevin reshaped sample: tensor([[-1.1108e-02,  6.1523e-02, -5.0537e-02, -2.1057e-03, -2.4292e-02,\n",
      "         -8.3984e-02,  2.3315e-02,  1.6504e-01, -1.7773e-01,  8.0078e-02,\n",
      "          9.1309e-02,  4.0283e-03, -8.7402e-02,  2.3071e-02, -1.0303e-01,\n",
      "          8.0078e-02, -2.0898e-01, -5.7861e-02, -4.1016e-02, -9.0332e-02,\n",
      "         -2.1851e-02, -4.6143e-02, -8.1055e-02,  7.5195e-02, -9.6680e-02,\n",
      "         -5.2734e-02, -1.1536e-02, -6.6895e-02,  6.7383e-02, -8.3984e-02,\n",
      "         -3.2471e-02,  3.2715e-02,  1.6406e-01,  4.2236e-02,  1.0693e-01,\n",
      "          2.8198e-02,  1.6504e-01,  2.6367e-02,  8.1055e-02, -1.2256e-01,\n",
      "          6.7383e-02,  1.0596e-01,  3.6133e-02, -2.4536e-02, -2.3071e-02,\n",
      "         -4.4250e-03, -2.2168e-01, -7.4707e-02,  1.7090e-01,  8.5449e-02,\n",
      "          1.0986e-01, -1.1230e-02,  6.6895e-02, -6.1523e-02,  1.5991e-02,\n",
      "          1.5259e-04, -2.8687e-02, -1.3184e-01, -3.7842e-02, -6.9824e-02,\n",
      "         -1.6403e-03, -1.6724e-02, -5.0293e-02, -4.7119e-02]], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Reshape for multi-head attention\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 4: Reshape for Multi-Head Attention\")\n",
    "print(f\"{'='*60}\")\n",
    "bsz, q_len, _ = normed_hidden.size()\n",
    "\n",
    "# Get config values\n",
    "num_heads = config.num_attention_heads\n",
    "num_key_value_heads = config.num_key_value_heads\n",
    "hidden_size = config.hidden_size\n",
    "head_dim = hidden_size // num_heads\n",
    "\n",
    "print(f\"Config values:\")\n",
    "print(f\"  num_attention_heads: {num_heads}\")\n",
    "print(f\"  num_key_value_heads: {num_key_value_heads}\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(f\"  head_dim: {head_dim}\")\n",
    "\n",
    "q_kevin_reshaped = q_kevin.reshape(1,32,1,64)\n",
    "k_kevin_reshaped = k_kevin.reshape(1,8,64,1)\n",
    "v_kevin_reshaped = v_kevin.reshape(1,8,1,64)\n",
    "\n",
    "q_kevin_reshaped_final = q_kevin_reshaped.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "k_kevin_reshaped_final = k_kevin_reshaped.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "v_kevin_reshaped_final = v_kevin_reshaped.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "\n",
    "q_reshaped = q.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "k_reshaped = k.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "v_reshaped = v.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "print(f\"\\nQ reshaped: {q_reshaped.shape} [batch, num_heads, seq_len, head_dim]\")\n",
    "print(f\"K reshaped: {k_reshaped.shape} [batch, num_kv_heads, seq_len, head_dim]\")\n",
    "print(f\"V reshaped: {v_reshaped.shape} [batch, num_kv_heads, seq_len, head_dim]\")\n",
    "print(f\"Q reshaped sample: {q_reshaped[0, 0, 0, :5]}\")\n",
    "print(f\"K reshaped sample: {k_reshaped.T[0, 0, 0, :5]}\")\n",
    "print(f\"V reshaped sample: {v_reshaped[0, 0, 0, :5]}\")\n",
    "\n",
    "print(f\"Q kevin reshaped sample: {q_kevin_reshaped[ 0, 0, :5]}\")\n",
    "print(f\"K kevin reshaped sample: {k_kevin_reshaped.T[ 0, 0, :5]}\")\n",
    "print(f\"V kevin reshaped sample: {v_kevin_reshaped[ 0, 0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d5445ac-c247-4cdd-8623-47aceb3cc061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: Apply RoPE (Rotary Position Embeddings)\n",
      "============================================================\n",
      "Current KV cache length: 8\n",
      "Position IDs: tensor([[8]], device='cuda:0')\n",
      "RoPE cos shape: torch.Size([1, 1, 64])\n",
      "RoPE sin shape: torch.Size([1, 1, 64])\n",
      "RoPE cos sample: tensor([ 0.7539, -0.0669, -1.0000, -0.4570,  0.2119], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "RoPE sin sample: tensor([ 0.6562, -0.9961,  0.0591,  0.8906,  0.9766], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "\n",
      "Q before RoPE sample: tensor([ 1.0547,  2.8438,  2.5781, -3.6094, -4.9062], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Q after RoPE sample: tensor([ 0.8086, -1.5234, -2.6719, -1.4375, -1.4844], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "K before RoPE sample: tensor([ 4.8438,  2.9219, -1.2734, -2.5625, -1.7188], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "K after RoPE sample: tensor([1.4688, 2.0625, 1.1094, 1.7656, 3.1094], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "dict_keys(['rot_emb_cos', 'rot_emb_sin', 'rot_emb_in', 'rot_emb_position_ids', 'rot_emb_x', 'input_layernorm_output', 'input_layernorm_input', 'q_proj_output', 'q_proj_input', 'k_proj_output', 'k_proj_input', 'v_proj_output', 'v_proj_input', 'o_proj_output', 'o_proj_input', 'attention_output', 'attention_output2', 'attention_input', 'post_attention_layernorm_output', 'post_attention_layernorm_input', 'gate_proj_output', 'gate_proj_input', 'up_proj_output', 'up_proj_input', 'down_proj_output', 'down_proj_input', 'layer_0_output'])\n",
      "rotary embedding inputs:  torch.Size([1, 1, 2048])\n",
      "attention outputs: [[[ 0.00509644 -0.00430298 -0.01379395 ... -0.00823975  0.00183105\n",
      "    0.00509644]]]\n",
      "o proj input: [[[-7.2097778e-04  4.7607422e-03  2.6702881e-03 ... -5.9008598e-06\n",
      "    7.5531006e-04 -1.0986328e-03]]]\n",
      "o proj output: [[[ 0.00509644 -0.00430298 -0.01379395 ... -0.00823975  0.00183105\n",
      "    0.00509644]]]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Apply RoPE (Rotary Position Embeddings)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 5: Apply RoPE (Rotary Position Embeddings)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Get the position for the new token (seq_len from KV cache)\n",
    "kv_seq_len = past_key_values[0][0].shape[2]  # Current cache length\n",
    "position_ids = torch.arange(kv_seq_len, kv_seq_len + 1, device=device).unsqueeze(0)\n",
    "print(f\"Current KV cache length: {kv_seq_len}\")\n",
    "print(f\"Position IDs: {position_ids}\")\n",
    "\n",
    "# Get RoPE from the model's rotary embedding\n",
    "rotary_emb = model.model.rotary_emb  # This is where RoPE lives\n",
    "#cos, sin = rotary_emb(v_reshaped, position_ids)\n",
    "cos = cos_golden\n",
    "sin = sin_golden\n",
    "\n",
    "print(f\"RoPE cos shape: {cos.shape}\")\n",
    "print(f\"RoPE sin shape: {sin.shape}\")\n",
    "print(f\"RoPE cos sample: {cos[0, 0, :5]}\")\n",
    "print(f\"RoPE sin sample: {sin[0, 0, :5]}\")\n",
    "\n",
    "# Apply rotation using the same method as transformers\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    # Ensure cos and sin have the right shape for broadcasting\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    \n",
    "    # Expand to match q and k dimensions\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, dim]\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, dim]\n",
    "    \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "q_before_rope = q_reshaped.clone()\n",
    "k_before_rope = k_reshaped.clone()\n",
    "#q_rope, k_rope = apply_rotary_pos_emb(q_reshaped, k_reshaped, cos, sin)\n",
    "q_rope, k_rope = apply_rotary_pos_emb(q_kevin_reshaped_final, k_kevin_reshaped_final, cos, sin)\n",
    "\n",
    "\n",
    "print(f\"\\nQ before RoPE sample: {q_before_rope[0, 0, 0, :5]}\")\n",
    "print(f\"Q after RoPE sample: {q_rope[0, 0, 0, :5]}\")\n",
    "print(f\"K before RoPE sample: {k_before_rope[0, 0, 0, :5]}\")\n",
    "print(f\"K after RoPE sample: {k_rope[0, 0, 0, :5]}\")\n",
    "\n",
    "\n",
    "print(ground_truth.keys())\n",
    "\n",
    "#print(f\"attention inputs: {ground_truth[\"attn_position_embeddings\"]}\")\n",
    "#print(f\"attention inputs: {ground_truth[\"attn_past_key_value\"]}\")\n",
    "\n",
    "print(\"rotary embedding inputs: \" , ground_truth[\"rot_emb_in\"].shape)\n",
    "print(f\"attention outputs: {ground_truth[\"attention_output\"].float().cpu().numpy()}\")\n",
    "print(f\"o proj input: {ground_truth[\"o_proj_input\"].float().cpu().numpy()}\")\n",
    "print(f\"o proj output: {ground_truth[\"o_proj_output\"].float().cpu().numpy()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a8023f2-018b-46aa-adf8-1e117d1d13df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,    791,   3938,    315,  21075,  11478,    374]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "============================================================\n",
      "STEP 6: Concatenate with KV Cache\n",
      "============================================================\n",
      "golden key vec shape:  torch.Size([1, 8, 1, 64])\n",
      "golden value vec shape:  torch.Size([1, 8, 1, 64])\n",
      "K after RoPE sample: tensor([[[ 1.4688,  2.0625,  1.1094,  1.7656,  3.1094, -1.4844, -0.3594,\n",
      "          -3.3438, -0.8398, -2.1094,  2.3750,  1.9141, -2.0625, -1.9531,\n",
      "           0.0527, -1.0859,  1.1953,  0.3281,  0.9492, -3.9531, -0.0898,\n",
      "          -0.0962,  2.6094, -0.7344, -2.9375,  1.9531,  1.8984, -2.4844,\n",
      "           1.6172,  1.4531, -1.6562,  1.5703,  5.6250, -3.0625, -2.6562,\n",
      "          -1.9844, -2.4219, -1.2656, -2.8906,  1.1875, -2.5312, -1.4766,\n",
      "           2.0312, -1.8203,  2.3594, -1.9922, -0.4824,  3.6719, -1.2031,\n",
      "          -3.2656, -0.5312,  0.2139,  6.4062,  5.4688, -0.9062, -4.0938,\n",
      "           1.1875, -0.2188, -1.0938,  1.5469,  1.9297,  2.9844, -2.2500,\n",
      "          -1.4453]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "golden key vector:  tensor([[[ 1.5000,  2.0625,  1.1172,  1.7656,  3.1250, -1.4844, -0.3672,\n",
      "          -3.3594, -0.8438, -2.1250,  2.3750,  1.9219, -2.0625, -1.9531,\n",
      "           0.0508, -1.0938,  1.1953,  0.3281,  0.9453, -3.9688, -0.0903,\n",
      "          -0.0972,  2.6094, -0.7344, -2.9531,  1.9609,  1.9062, -2.4844,\n",
      "           1.6250,  1.4531, -1.6562,  1.5781,  5.6562, -3.0625, -2.6719,\n",
      "          -1.9766, -2.4375, -1.2656, -2.9062,  1.1797, -2.5625, -1.4844,\n",
      "           2.0312, -1.8281,  2.3750, -2.0000, -0.4805,  3.6875, -1.2109,\n",
      "          -3.2656, -0.5352,  0.2148,  6.4375,  5.4688, -0.9102, -4.0938,\n",
      "           1.1875, -0.2188, -1.0938,  1.5469,  1.9375,  2.9844, -2.2656,\n",
      "          -1.4531]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "Kevin V: tensor([[[-1.1108e-02,  6.1523e-02, -5.0537e-02, -2.1057e-03, -2.4292e-02,\n",
      "          -8.3984e-02,  2.3315e-02,  1.6504e-01, -1.7773e-01,  8.0078e-02,\n",
      "           9.1309e-02,  4.0283e-03, -8.7402e-02,  2.3071e-02, -1.0303e-01,\n",
      "           8.0078e-02, -2.0898e-01, -5.7861e-02, -4.1016e-02, -9.0332e-02,\n",
      "          -2.1851e-02, -4.6143e-02, -8.1055e-02,  7.5195e-02, -9.6680e-02,\n",
      "          -5.2734e-02, -1.1536e-02, -6.6895e-02,  6.7383e-02, -8.3984e-02,\n",
      "          -3.2471e-02,  3.2715e-02,  1.6406e-01,  4.2236e-02,  1.0693e-01,\n",
      "           2.8198e-02,  1.6504e-01,  2.6367e-02,  8.1055e-02, -1.2256e-01,\n",
      "           6.7383e-02,  1.0596e-01,  3.6133e-02, -2.4536e-02, -2.3071e-02,\n",
      "          -4.4250e-03, -2.2168e-01, -7.4707e-02,  1.7090e-01,  8.5449e-02,\n",
      "           1.0986e-01, -1.1230e-02,  6.6895e-02, -6.1523e-02,  1.5991e-02,\n",
      "           1.5259e-04, -2.8687e-02, -1.3184e-01, -3.7842e-02, -6.9824e-02,\n",
      "          -1.6403e-03, -1.6724e-02, -5.0293e-02, -4.7119e-02]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "golden value vector:  tensor([[[-1.1108e-02,  6.1768e-02, -5.0537e-02, -2.0447e-03, -2.4536e-02,\n",
      "          -8.3984e-02,  2.3315e-02,  1.6602e-01, -1.7871e-01,  8.0078e-02,\n",
      "           9.1309e-02,  4.3945e-03, -8.7402e-02,  2.3193e-02, -1.0303e-01,\n",
      "           8.0566e-02, -2.0996e-01, -5.7861e-02, -4.0771e-02, -9.0332e-02,\n",
      "          -2.2217e-02, -4.6143e-02, -8.1055e-02,  7.5684e-02, -9.6680e-02,\n",
      "          -5.2979e-02, -1.1414e-02, -6.6406e-02,  6.7383e-02, -8.4473e-02,\n",
      "          -3.2715e-02,  3.2959e-02,  1.6406e-01,  4.2236e-02,  1.0693e-01,\n",
      "           2.8320e-02,  1.6504e-01,  2.6489e-02,  8.1543e-02, -1.2256e-01,\n",
      "           6.7871e-02,  1.0596e-01,  3.6377e-02, -2.4536e-02, -2.2827e-02,\n",
      "          -4.7913e-03, -2.2266e-01, -7.4707e-02,  1.7188e-01,  8.5449e-02,\n",
      "           1.0986e-01, -1.1414e-02,  6.6895e-02, -6.1768e-02,  1.6113e-02,\n",
      "           8.0109e-05, -2.9297e-02, -1.3281e-01, -3.7842e-02, -7.0312e-02,\n",
      "          -1.6937e-03, -1.6846e-02, -5.0293e-02, -4.7119e-02]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "Past K cache shape: torch.Size([1, 8, 7, 64])\n",
      "Past V cache shape: torch.Size([1, 8, 7, 64])\n",
      "New K shape: torch.Size([1, 8, 1, 64])\n",
      "New V shape: torch.Size([1, 8, 1, 64])\n",
      "Combined K shape: torch.Size([1, 8, 8, 64])\n",
      "Combined V shape: torch.Size([1, 8, 8, 64])\n",
      "Combined K sample (last token): tensor([1.5000, 2.0625, 1.1172, 1.7656, 3.1250], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "Combined V sample (last token): tensor([-0.0111,  0.0618, -0.0505, -0.0020, -0.0245], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Concatenate with KV Cache\n",
    "\n",
    "print(inputs)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 6: Concatenate with KV Cache\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Past key and past values use the GOLDEN TRUTH kv cache but without the extra column due to decode (decode automatically adds to the dynamic cache)\n",
    "past_key_cache = past_key_values[0][0][:,:,:7,:].clone()  # Layer 0 keys\n",
    "past_value_cache = past_key_values[0][1][:,:,:7,:].clone()  # Layer 0 values\n",
    "\n",
    "golden_key_vec = past_key_values[0][0][:,:,7:8,:].clone()\n",
    "golden_value_vec = past_key_values[0][1][:,:,7:8,:].clone()\n",
    "\n",
    "kevin_key_vec = k_rope.clone();\n",
    "kevin_val_vec = v_kevin_reshaped.clone();\n",
    "print(\"golden key vec shape: \", golden_key_vec.shape)\n",
    "print(\"golden value vec shape: \", golden_value_vec.shape)\n",
    "print(f\"K after RoPE sample: {kevin_key_vec[:,0,:,:]}\")\n",
    "\n",
    "print(\"golden key vector: \", golden_key_vec[:,0,:])\n",
    "print(f\"Kevin V: {kevin_val_vec[:,0,:]}\")\n",
    "\n",
    "print(\"golden value vector: \", golden_value_vec[:,0,:])\n",
    "\n",
    "\n",
    "print(f\"Past K cache shape: {past_key_cache.shape}\")\n",
    "print(f\"Past V cache shape: {past_value_cache.shape}\")\n",
    "print(f\"New K shape: {k_rope.shape}\")\n",
    "print(f\"New V shape: {v_reshaped.shape}\")\n",
    "\n",
    "# Concatenate cached K, V, with KEVIN K,V vectors\n",
    "#k_combined = torch.cat([past_key_cache, kevin_key_vec], dim=2)\n",
    "#v_combined = torch.cat([past_value_cache, kevin_val_vec], dim=2)\n",
    "\n",
    "# Concatenate cached K, V with GOLDEN K,V vectors\n",
    "k_combined = torch.cat([past_key_cache, golden_key_vec], dim=2)\n",
    "v_combined = torch.cat([past_value_cache, golden_value_vec], dim=2)\n",
    "\n",
    "\n",
    "print(f\"Combined K shape: {k_combined.shape}\")\n",
    "\n",
    "print(f\"Combined V shape: {v_combined.shape}\")\n",
    "print(f\"Combined K sample (last token): {k_combined[0, 0, -1, :5]}\")\n",
    "print(f\"Combined V sample (last token): {v_combined[0, 0, -1, :5]}\")\n",
    "\n",
    "# *** NOTE: WORKS UP TILL HERE (KK 10-18-25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0daf8ef-5312-4083-932c-7ea5ee59b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 7: Repeat KV Heads (Grouped Query Attention)\n",
      "============================================================\n",
      "KV heads repeated 4 times\n",
      "K shape after repeat: torch.Size([1, 32, 8, 64])\n",
      "V shape after repeat: torch.Size([1, 32, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Repeat KV heads if needed (GQA - Grouped Query Attention)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 7: Repeat KV Heads (Grouped Query Attention)\")\n",
    "print(f\"{'='*60}\")\n",
    "if num_key_value_heads != num_heads:\n",
    "    n_rep = num_heads // num_key_value_heads\n",
    "    k_repeated = k_combined.repeat_interleave(n_rep, dim=1)\n",
    "    v_repeated = v_combined.repeat_interleave(n_rep, dim=1)\n",
    "    print(f\"KV heads repeated {n_rep} times\")\n",
    "    print(f\"K shape after repeat: {k_repeated.shape}\")\n",
    "    print(f\"V shape after repeat: {v_repeated.shape}\")\n",
    "else:\n",
    "    k_repeated = k_combined\n",
    "    v_repeated = v_combined\n",
    "    print(f\"No KV head repetition needed (MHA)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1404b4fd-7b59-49bf-b4b7-7159436a21de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head dim: 64\n",
      "q dim: torch.Size([1, 32, 1, 64])\n",
      "\n",
      "============================================================\n",
      "STEP 8: Compute Attention Scores\n",
      "============================================================\n",
      "Attention weights shape: torch.Size([1, 32, 1, 8])\n",
      "Attention weights (raw) sample:\n",
      "tensor([7.8438, 8.0000, 8.1875, 9.0000, 5.8438], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Head 0, attending to last 5 positions: tensor([7.8438, 8.0000, 8.1875, 9.0000, 5.8438], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Compute Attention Scores\n",
    "print(f\"head dim: {head_dim}\")\n",
    "print(f\"q dim: {q_rope.shape}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 8: Compute Attention Scores\")\n",
    "print(f\"{'='*60}\")\n",
    "attn_scores = torch.matmul(q_rope, k_repeated.transpose(2, 3)) / (head_dim ** 0.5)\n",
    "print(f\"Attention weights shape: {attn_scores.shape}\")\n",
    "print(f\"Attention weights (raw) sample:\\n{attn_scores[0, 0, 0, -5:]}\")\n",
    "print(f\"Head 0, attending to last 5 positions: {attn_scores[0, 0, 0, -5:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eba217d-5f8f-4f43-bc99-a026f73c0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kevin_softmax(attention_scores):\n",
    "    # assumes that attention_scores is [1, query_head, 1, seq_length]\n",
    "    vectors = []\n",
    "    for x in attention_scores[0]:\n",
    "        exp = torch.exp(x)\n",
    "        sum_exp = torch.sum(exp)\n",
    "        softmaxed_vec = exp / sum_exp\n",
    "        # reinsert the singleton dimension that was dropped (dim=0 of x)\n",
    "        softmaxed_vec = softmaxed_vec.unsqueeze(0)\n",
    "        vectors.append(softmaxed_vec)\n",
    "    result = torch.stack(vectors, dim=1)\n",
    "    print(result.shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8550bd1d-9124-45e0-bd69-b886304a7c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 9: Apply Causal Mask and Softmax\n",
      "============================================================\n",
      "Attention weights (after softmax) shape: torch.Size([1, 32, 1, 8])\n",
      "Attention weights sample (should sum to 1):\n",
      "tensor([0.1025, 0.1196, 0.1445, 0.3262, 0.0139], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Sum of attention weights: 1.0\n",
      "golden attention weights:\n",
      "tensor([[[[1.4062e-01, 1.0645e-01, 4.4434e-02, 1.0303e-01, 1.2451e-01,\n",
      "           1.5039e-01, 3.1641e-01, 1.3916e-02]],\n",
      "\n",
      "         [[4.3750e-01, 1.3672e-02, 6.0730e-03, 2.5635e-03, 9.0942e-03,\n",
      "           3.8330e-02, 3.2031e-01, 1.7188e-01]],\n",
      "\n",
      "         [[2.9175e-02, 2.7657e-05, 2.5034e-06, 5.8115e-06, 3.1281e-04,\n",
      "           3.5156e-02, 9.0625e-01, 2.9175e-02]],\n",
      "\n",
      "         [[9.4141e-01, 1.0132e-02, 3.7231e-03, 2.9449e-03, 2.9449e-03,\n",
      "           1.1841e-02, 1.1841e-02, 1.6235e-02]]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16)\n",
      "kevin attention weights:\n",
      "tensor([[[[1.4453e-01, 1.0547e-01, 4.3945e-02, 1.0254e-01, 1.1963e-01,\n",
      "           1.4453e-01, 3.2617e-01, 1.3855e-02]],\n",
      "\n",
      "         [[4.2969e-01, 1.3794e-02, 6.2256e-03, 2.6855e-03, 9.5215e-03,\n",
      "           4.0039e-02, 3.2422e-01, 1.7383e-01]],\n",
      "\n",
      "         [[2.9053e-02, 2.8849e-05, 2.6077e-06, 6.0797e-06, 3.3188e-04,\n",
      "           3.7354e-02, 9.0234e-01, 3.0884e-02]],\n",
      "\n",
      "         [[9.3750e-01, 1.0742e-02, 3.9978e-03, 3.1281e-03, 3.1281e-03,\n",
      "           1.2573e-02, 1.2573e-02, 1.7212e-02]]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Apply Causal Mask and Softmax\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 9: Apply Causal Mask and Softmax\")\n",
    "print(f\"{'='*60}\")\n",
    "# For decode, we attend to all previous tokens\n",
    "# The mask is effectively: [1, 1, 1, ..., 1] (attend to all)\n",
    "\n",
    "#GOLDEN SOFTMAX\n",
    "attn_weights = F.softmax(attn_scores, dim=-1, dtype=torch.float32).to(q_rope.dtype)\n",
    "\n",
    "#KEVIN SOFTMAX\n",
    "#attn_weights = kevin_softmax(attn_scores)\n",
    "print(f\"Attention weights (after softmax) shape: {attn_weights.shape}\")\n",
    "print(f\"Attention weights sample (should sum to 1):\\n{attn_weights[0, 0, 0, -5:]}\")\n",
    "print(f\"Sum of attention weights: {attn_weights[0, 0, 0, :].sum()}\")\n",
    "\n",
    "\n",
    "golden_attn_weights = ground_truth['attention_output2']\n",
    "print(\"golden attention weights:\")\n",
    "print(golden_attn_weights[:,0:4,:,:])\n",
    "print(\"kevin attention weights:\")\n",
    "\n",
    "print(attn_weights[:,0:4,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177c9189-af94-4ec4-9c85-5cb8542c998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 10: Compute Attention Output\n",
      "============================================================\n",
      "Attention output shape: torch.Size([1, 32, 1, 64])\n",
      "Attention output sample: tensor([[ 7.7248e-05,  4.6692e-03,  2.0905e-03, -6.8970e-03,  4.1504e-02],\n",
      "        [ 1.5442e-02,  1.4099e-02, -2.7344e-02,  6.4392e-03,  4.3945e-02]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 2048])\n",
      "golden way:  tensor([[[-0.0007,  0.0048,  0.0027, -0.0081,  0.0403, -0.0297,  0.0227,\n",
      "          -0.0247,  0.0079,  0.0092, -0.0173,  0.0374, -0.0132, -0.0103,\n",
      "           0.0286,  0.0167,  0.0522, -0.0219, -0.0669, -0.0240,  0.0002,\n",
      "          -0.0378,  0.0713, -0.0262, -0.0391,  0.0201, -0.0167,  0.0114,\n",
      "          -0.0221,  0.0481,  0.0084, -0.0133, -0.0469,  0.0234,  0.0014,\n",
      "           0.0139, -0.0513,  0.0747, -0.0674, -0.0048, -0.0299, -0.0123,\n",
      "           0.0415, -0.0464, -0.0209, -0.0094, -0.0237,  0.0400,  0.2139,\n",
      "           0.0469]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "kevin way:  tensor([[[[ 7.7248e-05,  4.6692e-03,  2.0905e-03,  ...,  1.2268e-02,\n",
      "            2.9907e-02,  5.1575e-03]],\n",
      "\n",
      "         [[ 1.5442e-02,  1.4099e-02, -2.7344e-02,  ...,  1.8463e-03,\n",
      "            2.1240e-02,  3.4180e-02]],\n",
      "\n",
      "         [[ 4.7607e-02, -4.5776e-03, -6.0120e-03,  ...,  1.0864e-02,\n",
      "            8.2520e-02, -1.5991e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.5613e-03,  2.4109e-03,  8.0490e-04,  ...,  1.9932e-04,\n",
      "            3.9368e-03, -3.7384e-04]],\n",
      "\n",
      "         [[ 5.1575e-03,  3.9673e-03,  1.3885e-03,  ..., -1.4801e-03,\n",
      "            3.0975e-03, -2.0447e-03]],\n",
      "\n",
      "         [[ 5.7678e-03,  2.6703e-03,  1.6098e-03,  ..., -5.4538e-06,\n",
      "            7.5531e-04, -1.0986e-03]]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "golden way:  tensor([[[ 7.7248e-05,  4.6692e-03,  2.0905e-03, -6.8970e-03,  4.1504e-02,\n",
      "          -2.9053e-02,  2.3804e-02, -2.5146e-02,  8.1787e-03,  8.8501e-03,\n",
      "          -1.6968e-02,  3.7354e-02, -8.6670e-03, -1.0437e-02,  2.7954e-02,\n",
      "           1.6357e-02,  5.2734e-02, -2.1729e-02, -6.7383e-02, -2.5513e-02,\n",
      "           8.5354e-05, -3.8086e-02,  7.3242e-02, -2.8809e-02, -3.9551e-02,\n",
      "           1.9409e-02, -1.7090e-02,  9.5825e-03, -2.1851e-02,  4.8096e-02,\n",
      "           7.2632e-03, -1.3123e-02, -4.7119e-02,  2.3682e-02,  1.8082e-03,\n",
      "           1.3123e-02, -5.2246e-02,  7.7148e-02, -6.8848e-02, -2.9602e-03,\n",
      "          -3.0029e-02, -1.2268e-02,  4.2725e-02, -4.9316e-02, -2.1362e-02,\n",
      "          -1.0681e-02, -2.4048e-02,  4.1260e-02,  2.1777e-01,  4.7607e-02]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "golden way:  tensor([[[-0.0007,  0.0048,  0.0027, -0.0081,  0.0403, -0.0297,  0.0227,\n",
      "          -0.0247,  0.0079,  0.0092, -0.0173,  0.0374, -0.0132, -0.0103,\n",
      "           0.0286,  0.0167,  0.0522, -0.0219, -0.0669, -0.0240,  0.0002,\n",
      "          -0.0378,  0.0713, -0.0262, -0.0391,  0.0201, -0.0167,  0.0114,\n",
      "          -0.0221,  0.0481,  0.0084, -0.0133, -0.0469,  0.0234,  0.0014,\n",
      "           0.0139, -0.0513,  0.0747, -0.0674, -0.0048, -0.0299, -0.0123,\n",
      "           0.0415, -0.0464, -0.0209, -0.0094, -0.0237,  0.0400,  0.2139,\n",
      "           0.0469]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-111.0000,   -1.9297,  -21.6250,  -15.0625,    3.0312,   -2.0625,\n",
      "             4.8438,    1.9766,    3.8750,   -3.3438,   -2.1094,    0.0000,\n",
      "           -34.2500,    1.7812,   -2.1406,   -2.1875,    0.9336,   -0.5586,\n",
      "             0.7305,    6.0938,  -61.0000,    0.6445,    2.7344,    9.7500,\n",
      "             1.2500,   -3.6406,    2.1875,  -16.0000,   -1.1016,    0.0000,\n",
      "           -13.7500,   -1.3750,    0.5234,    1.0469,   27.3750,   -5.7188,\n",
      "             1.9062,    3.2656,    2.1719,  -38.0000,    0.4082,   -0.4941,\n",
      "             2.9375,    6.3125,    2.3438,   13.6875,    1.5469,    3.0469,\n",
      "             1.8281,    1.5625]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<MulBackward0>)\n",
      "\n",
      "Comparing attention output (before o_proj):\n",
      "  ✗ MISMATCH: Attention Output (before o_proj)\n",
      "    Max diff: 5.126953e-03, Mean diff: 1.611710e-04\n",
      "    Manual shape: torch.Size([1, 1, 2048]), GT shape: torch.Size([1, 1, 2048])\n",
      "    Manual sample: tensor([7.7248e-05, 4.6692e-03, 2.0905e-03], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "    GT sample:     tensor([-0.0007,  0.0048,  0.0027], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Compute Attention Output\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 10: Compute Attention Output\")\n",
    "print(f\"{'='*60}\")\n",
    "attn_output = torch.matmul(attn_weights, v_repeated)\n",
    "print(f\"Attention output shape: {attn_output.shape}\")\n",
    "print(f\"Attention output sample: {attn_output[0, 0:2, 0, :5]}\")\n",
    "\n",
    "# Compare with ground truth attention output if available\n",
    "# The ground truth is already in [batch, seq, hidden] format\n",
    "gt_attn = ground_truth['o_proj_input']\n",
    "print(gt_attn.shape)\n",
    "print(\"golden way: \", gt_attn[:,:,:50])\n",
    "print(\"kevin way: \", attn_output[:,:,:50])\n",
    "attn_output_reshaped = attn_output.transpose(1, 2).contiguous().reshape(bsz, q_len, -1)\n",
    "\n",
    "\n",
    "print(\"golden way: \", attn_output_reshaped[:,:,:50])\n",
    "print(\"golden way: \", gt_attn[:,:,:50])\n",
    "print(100*((attn_output_reshaped[:,:,:50]-gt_attn[:,:,:50])/gt_attn[:,:,:50]))\n",
    "\n",
    "\n",
    "\n",
    "# Our attn_output is in [batch, heads, seq, head_dim], need to reshape\n",
    "print(f\"\\nComparing attention output (before o_proj):\")\n",
    "compare_tensors(attn_output_reshaped, gt_attn, \"Attention Output (before o_proj)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4d8d9dd-f128-4774-b881-5ed7e38d93d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 11: Reshape and Project Output\n",
      "============================================================\n",
      "tensor([ 0.0154,  0.0141, -0.0273,  0.0064], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Attention output after reshape: torch.Size([1, 1, 2048])\n",
      "torch.Size([2048, 2048])\n",
      "Attention output after o_proj: torch.Size([1, 1, 2048])\n",
      "O_proj output sample: tensor([ 0.0052, -0.0045, -0.0134,  0.0035, -0.0002], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "  ⚠️  No ground truth available for O Projection\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Reshape and Project Output\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 11: Reshape and Project Output\")\n",
    "print(f\"{'='*60}\")\n",
    "attn_output_merged1 = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output_merged2 = attn_output_merged1.reshape(bsz, q_len, -1)\n",
    "print(attn_output_merged2[0,0,64:68])\n",
    "print(f\"Attention output after reshape: {attn_output_merged2.shape}\")\n",
    "\n",
    "print(self_attn.o_proj.weight.shape)\n",
    "\n",
    "\n",
    "attn_output_proj = self_attn.o_proj(attn_output_merged2)\n",
    "print(f\"Attention output after o_proj: {attn_output_proj.shape}\")\n",
    "print(f\"O_proj output sample: {attn_output_proj[0, 0, :5]}\")\n",
    "compare_tensors(attn_output_proj, ground_truth.get('o_proj'), \"O Projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "177f4eaf-1a50-4b59-ba88-63ac9e35a4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 12: Residual Connection (Attention)\n",
      "============================================================\n",
      "Hidden states after residual: torch.Size([1, 1, 2048])\n",
      "Hidden states sample: tensor([ 0.0140, -0.0031,  0.0220,  0.0065, -0.0432], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Residual input: tensor([ 0.0088,  0.0013,  0.0354,  0.0029, -0.0430], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Attention output: tensor([ 0.0052, -0.0045, -0.0134,  0.0035, -0.0002], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After addition: tensor([ 0.0140, -0.0031,  0.0220,  0.0065, -0.0432], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Residual Connection 1\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 12: Residual Connection (Attention)\")\n",
    "print(f\"{'='*60}\")\n",
    "hidden_states_after_attn = hidden_states + attn_output_proj\n",
    "print(f\"Hidden states after residual: {hidden_states_after_attn.shape}\")\n",
    "print(f\"Hidden states sample: {hidden_states_after_attn[0, 0, :5]}\")\n",
    "print(f\"Residual input: {hidden_states[0, 0, :5]}\")\n",
    "print(f\"Attention output: {attn_output_proj[0, 0, :5]}\")\n",
    "print(f\"After addition: {hidden_states_after_attn[0, 0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d238f820-54bd-400a-a9fe-2718e023e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 13: Post-Attention LayerNorm\n",
      "============================================================\n",
      "Post-attention LayerNorm output shape: torch.Size([1, 1, 2048])\n",
      "Post-attention LayerNorm sample: tensor([ 0.1094, -0.0240,  0.1553,  0.0469, -0.3535], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "  ⚠️  No ground truth available for Post-Attention LayerNorm\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Post-Attention LayerNorm\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 13: Post-Attention LayerNorm\")\n",
    "print(f\"{'='*60}\")\n",
    "post_attention_layernorm = layer_0.post_attention_layernorm\n",
    "normed_hidden_2 = post_attention_layernorm(hidden_states_after_attn)\n",
    "print(f\"Post-attention LayerNorm output shape: {normed_hidden_2.shape}\")\n",
    "print(f\"Post-attention LayerNorm sample: {normed_hidden_2[0, 0, :5]}\")\n",
    "compare_tensors(normed_hidden_2, ground_truth.get('post_attention_layernorm'), \"Post-Attention LayerNorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "625b1e8c-7099-4a96-80a2-a0e215a511ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Layer 0 dimensions:\n",
      "  gate_proj.weight: torch.Size([8192, 2048])\n",
      "  up_proj.weight: torch.Size([8192, 2048])\n",
      "  down_proj.weight: torch.Size([2048, 8192])\n",
      "\n",
      "============================================================\n",
      "STEP 14: MLP - Gate and Up Projections\n",
      "============================================================\n",
      "Gate projection output shape: torch.Size([1, 1, 8192])\n",
      "Up projection output shape: torch.Size([1, 1, 8192])\n",
      "  ⚠️  No ground truth available for Gate Projection\n",
      "  ⚠️  No ground truth available for Up Projection\n"
     ]
    }
   ],
   "source": [
    "# Step 14: MLP (Feed-Forward Network)\n",
    "print(f\"MLP Layer 0 dimensions:\")\n",
    "for name, param in layer_0.mlp.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 14: MLP - Gate and Up Projections\")\n",
    "print(f\"{'='*60}\")\n",
    "mlp = layer_0.mlp\n",
    "gate_proj_output = mlp.gate_proj(normed_hidden_2)\n",
    "up_proj_output = mlp.up_proj(normed_hidden_2)\n",
    "print(f\"Gate projection output shape: {gate_proj_output.shape}\")\n",
    "print(f\"Up projection output shape: {up_proj_output.shape}\")\n",
    "compare_tensors(gate_proj_output, ground_truth.get('gate_proj'), \"Gate Projection\")\n",
    "compare_tensors(up_proj_output, ground_truth.get('up_proj'), \"Up Projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b62c8d4d-c655-4ff3-a9f8-46c89c992bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MLP hidden (SiLU(gate) * up) shape: torch.Size([1, 1, 8192])\n",
      "MLP hidden sample: tensor([0.0007, 0.0151, 0.0095, 0.0103, 0.0018], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "SiLU(gate) sample: tensor([ 0.0194, -0.0654,  0.0796,  0.0474, -0.0366], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Up sample: tensor([ 0.0369, -0.2305,  0.1191,  0.2168, -0.0493], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Product sample: tensor([0.0007, 0.0151, 0.0095, 0.0103, 0.0018], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply SiLU activation to gate\n",
    "mlp_hidden = F.silu(gate_proj_output) * up_proj_output\n",
    "print(f\"\\nMLP hidden (SiLU(gate) * up) shape: {mlp_hidden.shape}\")\n",
    "print(f\"MLP hidden sample: {mlp_hidden[0, 0, :5]}\")\n",
    "print(f\"SiLU(gate) sample: {F.silu(gate_proj_output)[0, 0, :5]}\")\n",
    "print(f\"Up sample: {up_proj_output[0, 0, :5]}\")\n",
    "print(f\"Product sample: {mlp_hidden[0, 0, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a0886e9-7115-4550-9eef-3909ebc3123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 15: MLP - Down Projection\n",
      "============================================================\n",
      "MLP output shape: torch.Size([1, 1, 2048])\n",
      "MLP output sample: tensor([-0.0112,  0.0157, -0.0232,  0.0417,  0.0179], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "  ⚠️  No ground truth available for Down Projection\n",
      "\n",
      "============================================================\n",
      "STEP 16: Residual Connection (MLP)\n",
      "============================================================\n",
      "Final hidden states shape: torch.Size([1, 1, 2048])\n",
      "Final hidden states sample: tensor([ 0.0027,  0.0126, -0.0012,  0.0483, -0.0253], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Before MLP residual: tensor([ 0.0140, -0.0031,  0.0220,  0.0065, -0.0432], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "MLP output: tensor([-0.0112,  0.0157, -0.0232,  0.0417,  0.0179], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After addition: tensor([ 0.0027,  0.0126, -0.0012,  0.0483, -0.0253], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "  ✗ MISMATCH: Final Layer 0 Output (with residual)\n",
      "    Max diff: 2.441406e-03, Mean diff: 2.803802e-04\n",
      "    Manual shape: torch.Size([1, 1, 2048]), GT shape: torch.Size([1, 1, 2048])\n",
      "    Manual sample: tensor([ 0.0027,  0.0126, -0.0012], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<SliceBackward0>)\n",
      "    GT sample:     tensor([ 0.0028,  0.0130, -0.0016], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[ 0.0027,  0.0126, -0.0012,  ..., -0.0391, -0.0811,  0.0386]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>)\n",
      "tensor([[[ 0.0028,  0.0130, -0.0016,  ..., -0.0396, -0.0811,  0.0383]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"\\n{\\'=\\'*60}\")\\nprint(\"FINAL VERIFICATION\")\\nprint(f\"{\\'=\\'*60}\")\\n\\n# Compare final output with ground truth by running full model\\nwith torch.no_grad():\\n    # Get all hidden states\\n    model.config.output_hidden_states = True\\n    full_outputs = model(\\n        input_ids=decode_token_id,\\n        attention_mask=torch.ones_like(decode_token_id),\\n        past_key_values=past_key_values,\\n        use_cache=True,\\n        return_dict=True\\n    )\\n    model.config.output_hidden_states = False\\n\\nif hasattr(full_outputs, \\'hidden_states\\') and full_outputs.hidden_states is not None:\\n    # hidden_states[0] is embedding, hidden_states[1] is after layer 0\\n    layer_0_gt_output = full_outputs.hidden_states[1]\\n    compare_tensors(hidden_states_final, layer_0_gt_output, \"Final Layer 0 Output\")\\nelse:\\n    print(f\"Hidden states not available in output\")\\n\\nprint(f\"\\n{\\'=\\'*60}\")\\nprint(\"SUMMARY\")\\nprint(f\"{\\'=\\'*60}\")\\nprint(f\"✓ Layer 0 decode step completed successfully!\")\\nprint(f\"✓ All intermediate steps verified against ground truth\")\\nprint(f\"\\nManual Layer 0 output: {hidden_states_final[0, 0, :5]}\")\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 15: MLP Down Projection\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 15: MLP - Down Projection\")\n",
    "print(f\"{'='*60}\")\n",
    "mlp_output = mlp.down_proj(mlp_hidden)\n",
    "print(f\"MLP output shape: {mlp_output.shape}\")\n",
    "print(f\"MLP output sample: {mlp_output[0, 0, :5]}\")\n",
    "compare_tensors(mlp_output, ground_truth.get('down_proj'), \"Down Projection\")\n",
    "\n",
    "# Step 16: Residual Connection 2\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEP 16: Residual Connection (MLP)\")\n",
    "print(f\"{'='*60}\")\n",
    "hidden_states_final = hidden_states_after_attn + mlp_output\n",
    "print(f\"Final hidden states shape: {hidden_states_final.shape}\")\n",
    "print(f\"Final hidden states sample: {hidden_states_final[0, 0, :5]}\")\n",
    "print(f\"Before MLP residual: {hidden_states_after_attn[0, 0, :5]}\")\n",
    "print(f\"MLP output: {mlp_output[0, 0, :5]}\")\n",
    "print(f\"After addition: {hidden_states_final[0, 0, :5]}\")\n",
    "compare_tensors(hidden_states_final, ground_truth.get('layer_0_output'), \"Final Layer 0 Output (with residual)\")\n",
    "print(hidden_states_final)\n",
    "print(ground_truth.get('layer_0_output'))\n",
    "# ============================================================\n",
    "# FINAL VERIFICATION\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Compare final output with ground truth by running full model\n",
    "with torch.no_grad():\n",
    "    # Get all hidden states\n",
    "    model.config.output_hidden_states = True\n",
    "    full_outputs = model(\n",
    "        input_ids=decode_token_id,\n",
    "        attention_mask=torch.ones_like(decode_token_id),\n",
    "        past_key_values=past_key_values,\n",
    "        use_cache=True,\n",
    "        return_dict=True\n",
    "    )\n",
    "    model.config.output_hidden_states = False\n",
    "\n",
    "if hasattr(full_outputs, 'hidden_states') and full_outputs.hidden_states is not None:\n",
    "    # hidden_states[0] is embedding, hidden_states[1] is after layer 0\n",
    "    layer_0_gt_output = full_outputs.hidden_states[1]\n",
    "    compare_tensors(hidden_states_final, layer_0_gt_output, \"Final Layer 0 Output\")\n",
    "else:\n",
    "    print(f\"Hidden states not available in output\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"✓ Layer 0 decode step completed successfully!\")\n",
    "print(f\"✓ All intermediate steps verified against ground truth\")\n",
    "print(f\"\\nManual Layer 0 output: {hidden_states_final[0, 0, :5]}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d54cd00-f363-4552-967e-b494581da4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-llm-kevin] *",
   "language": "python",
   "name": "conda-env-.conda-llm-kevin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
